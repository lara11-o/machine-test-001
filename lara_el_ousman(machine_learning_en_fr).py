# -*- coding: utf-8 -*-
"""Copy of Lara El Ousman(Machine Learning -EN/FR).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKX1AKPMPSY-KD7UOIPXQvQnrHC6r05m

##Machine Translation Using a Seq2Seq Architecture
© 2022 Zaka AI, Inc. All Rights Reserved.

---
The goal of this colab is to get you more familiar with the Seq2Seq models and their challenges. For this reason, you will be working on machine translation problem where we would have a sentence as input (in english), and the output is gonna be the translated sentence (in french). So just like what happens with Google Translate.

**Just to give you a heads up:** We won't be having a model performing like Google translate, but at least we will have an idea about how Google Translate works and the challenges that exist with a translation problem.

## Importing Libraries

We start by importing numpy and pandas and then we can add the rest
"""

import pandas as pd
import numpy as np
import re
import string #used in removing punctuation
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, RepeatVector, Bidirectional
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from google.colab import files

!pip install gwpy --quiet
!pip install livelossplot --quiet
from livelossplot import PlotLossesKeras

# # Using PLT Library in general was not working and this solution worked
import matplotlib
if matplotlib.__version__ != '3.1.3' :
  ! pip uninstall matplotlib
  ! pip install matplotlib==3.1.3

"""We clone the github repository where our data exists. Here is the github link: https://github.com/zaka-ai/machine_learning_certification/tree/main/Challenge%207

## Getting the data
"""

df_en = pd.read_csv('https://raw.githubusercontent.com/zaka-ai/machine_learning_certification/main/Challenge%207/en.csv', header = None)
df_fr = pd.read_csv('https://raw.githubusercontent.com/zaka-ai/machine_learning_certification/main/Challenge%207/fr.csv', header = None)

"""We read the english sentences in a dataframe named "english", and the french sentences in a dataframe named "french"
"""

#Test Your Zaka (English)
df_en.columns = ['english']
df_en.head()

#Test Your Zaka (French)
df_fr.columns = ['french']
df_fr.head()

"""**How many sentences does each of the files contain?**"""

#Test Your Zaka
print(len(df_en))
len(df_fr)

"""Now let us concatenate the 2 dataframes into one dataframe that we call **df** where one column has the english senetnces and the other has the french sentences"""

#Test Your Zaka
# del df
df = pd.concat([df_en, df_fr], axis = 1)
# df = df.sort_values(by=['english', 'french']).reset_index(drop=True)
df.head()

"""Let's name the columns as **English** and **French** so that we access them easier."""

#Test Your Zaka
#DONE

"""Pick a sentence and print it in both languages"""

#Test Your Zaka
print(f'English: {df.english[1]}')
print(f'French {df.french[1]}')

"""##Cleaning Data

The data that we have is almost clean as we can see, we just need to remove the punctuations inside of it.
"""

df.english = df.english.apply(lambda x : re.sub(r'[^a-z]', ' ', x))
french_test = df.french.apply(lambda x : re.sub(r'[^a-z]', ' ', x))
#Test Your Zaka
print(f'English: {df.english[1]}')
print(f'French {french_test[1]}')

"""It was successful on the english text but **didnt** work with the **french** sentence. As we can notice, letters with special characters such as the acute **accent** é etc are removed. Thus, we used **string** library."""

print(string.punctuation)
t=[]
sen = []
for sentence in df.french:
  for word in sentence.split():
    sen.append(''.join( word.strip(string.punctuation)))
  t.append(' '.join(sen))
  sen = []
df.french = t

"""Make sure that the punctuation is removed by printing the example that you printed earlier."""

#Test Your Zaka
print(f'English: {df.english[1]}')
print(f'French {df.french[1]}')

"""##Exploring the Data

Add a column **ENG Length** to the dataset that shows how many words does a sentence contain, and do the same for french in a column called **FR Length**
"""

#Test Your Zaka
df['eng_length'] = df.english.apply(lambda x : len(x.split()))
df['fr_length'] = df.french.apply(lambda x : len(x.split()))
df

"""Visualize the distribution of the lengths of english sentences and french sentences."""

#Test Your Zaka
df.eng_length.hist()

#Test Your Zaka
df.fr_length.hist()

"""Get the maximum length of an english sentence and the maximum length of a french sentence. """

#Test Your Zaka
print(f'Max len of eng sentence : {max(df.eng_length)} and max in fr is : {max(df.fr_length)}')

"""##Preprocessing the Data

In order for the data to be fed to the model, it has to be tokenized and padded.

####Tokenization

**To tokenize english and french sentences, we can use only one tokenizer. True or False?**

One us not suitable for both languages as wach one differs than the other in many things such as stopwords etc and both languages has different words.

Tokenize the sentences that we have.
"""

#Test Your Zaka

# define your tokenizer
tokenizer_eng = Tokenizer()
tokenizer_fr = Tokenizer()

# assign an index (number) to each word using fit_on_texts function
tokenizer_eng.fit_on_texts(df.english)
tokenizer_fr.fit_on_texts(df.french)

# transform each text to a sequence of integers (to be used later in embeddings layer)
eng_token =  tokenizer_eng.texts_to_sequences(df.english)
fr_token =  tokenizer_fr.texts_to_sequences(df.french)

#Testing
print(tokenizer_eng.word_index)
eng_token[1]

"""**How many unique words do we have in english and in french?**"""

#Test Your Zaka
print(f'There are {len(tokenizer_eng.word_index)} unique eng words and {len(tokenizer_fr.word_index)} unique fr words')

"""####Padding

**What should be the length of the sequences that we have after padding?**

It is based on the longest sentence we have in both languages which is 15 in English and 21 in French

Perform padding on the sequences that we have.
"""

#Test Your Zaka

# apply post-padding to the sequences
eng_max_length = max(df.eng_length)
fr_max_length = max(df.fr_length)

eng_pad = pad_sequences(eng_token, maxlen=eng_max_length, padding='post')
fr_pad = pad_sequences(fr_token, maxlen=fr_max_length, padding='post')

print(fr_pad[1])
fr_pad[2]

"""##Modeling

After preprrocessing the data, we can build our model. Start by building a baseline architecture relying on one directional RNNs, LSTMs, or GRUs. It will be good to lookup how to build Seq2Seq models, there are some new layers that will help you like RepeatVector and TimeDistributed.
"""

eng_vocab_size = len(tokenizer_eng.word_index) + 1
fr_vocab_size = len(tokenizer_fr.word_index) + 1
embedding_dim = 256
hidden_size = 1024 # Just for testing purposes but not good idea for now to have a big number as the model size could go from few KBs to 100 MBs easily

# FILL BLANKS
# build the neural network
model = Sequential()

model.add(Embedding(eng_vocab_size, embedding_dim, input_length=eng_max_length))

model.add(LSTM(hidden_size))
model.add(RepeatVector(fr_max_length))

model.add(LSTM(hidden_size, return_sequences=True))
model.add(TimeDistributed(Dense(fr_vocab_size, activation='softmax')))

# THere is an error : when we use categorical_crossentropy
# Solutions: either use the sparse loss function or do one hot encoding to the input data
# the one hot encoding is sparse representation but it is used just when inputting data to the embedding layer thus we keep the dense representation
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.summary()

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

eng_pad.shape, fr_pad.shape, eng_vocab_size, fr_vocab_size, eng_max_length, fr_max_length

"""Compile and train the model. 
**FYI:** While specifying the architecture of your model and the number of epochs for training, keeep in your mind that your model might take A LOT of time to train.

We tested several Models which are:
- Model1 = normal LSTM
- Model2 = 2x Bi LSTM
- Model3 = 1x Bi LSTM
- Model4 = 1x Bi LSTM + data splitting
- Model5 = Stacking BiLSTMs


# **Model1 = normal LSTM**
"""

# EarlyStopping
early_stop = EarlyStopping(monitor='val_loss',
                           patience=10,
                           restore_best_weights=True,
                           mode='min')

#Test Your Zaka
model.fit(x = eng_pad, y = fr_pad, batch_size = 64, epochs=10, validation_split=0.15, callbacks=[PlotLossesKeras(), early_stop])

# Saving the model to avoid training it again due to runtime reset:
!mkdir -p saved_model

# Naming convention: translation_'model architecture used'_'loss'_'acc'
model.save('saved_model/translation_LSTM_0.0375_0.9881.h5')

#Load the model:
new_model = tf.keras.models.load_model('/content/saved_model/translation_LSTM_0.0375_0.9881.h5')

# Check its architecture
new_model.summary()

files.download('saved_model/translation_LSTM_0.0375_0.9881.h5')

"""Define a function that gets an input sentence in english and gives the output sentence in the french language."""

#Test Your Zaka
def translate(eng_sentence):
  print(eng_sentence)
  
  # transform each text to a sequence of integers (to be used later in embeddings layer)
  # This line "sentence_token =  tokenizer_eng.texts_to_sequences(eng_sentence)" didnt work as it apply tokenization to each word or letter in the string  
  # (Based on the documentation it takes each text in texts, so it seems that it considered the input string as texts and divides it )
  # As a result, converting the string to a list [] takes the full string as 1 sentence 
  sentence_token =  tokenizer_eng.texts_to_sequences([eng_sentence])
  # print(sentence_token) # [[27, 1, 107, 5, 101]]

  sentence_pad = pad_sequences(sentence_token, maxlen=eng_max_length, padding='post')

  prediction = model.predict(sentence_pad)
  
  prediction.shape # (1, 21, 345) = output of the Decoder
  translated_words = np.argmax(prediction, axis = 2) # argmax across axis 2 gives us (21, 345) where we have 21 max length sentence and 345 possible word
  no_zero = [i for i in translated_words[0] if i != 0] # removing zeros
  
  translated_sentence = [list(tokenizer_fr.word_index)[word - 1] for word in no_zero]
  
  translated_sentence = ' '.join(translated_sentence)
  return translated_sentence

"""Test the following sentence"""

input = "she is driving the truck"
print(input)

# # A working method to pass the input sentence to tokenizer_eng.texts_to_sequences() directly without using [] brackets
# from io import StringIO
# t = pd.read_csv(StringIO(input), header = None)
# t.columns = ['eng']
# t.head()
# # print(df.english[1])
#Test Your Zaka
# translate(t.eng)

prediction = translate(input)
prediction

"""Try to improve your model by modifying the architecture to take into account bidirectionality which is very useful in Machine Translation. Create a new model called model2

# **Model2 = 2x Bidirectional LSTM**
"""

#Test Your Zaka
embedding_dim = 256

model2 = Sequential()
model2.add(Embedding(eng_vocab_size, embedding_dim, input_length=eng_max_length))
model2.add(Bidirectional(LSTM(hidden_size)))
model2.add(RepeatVector(fr_max_length))
model2.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))
model2.add(TimeDistributed(Dense(fr_vocab_size, activation='softmax')))

model2.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model2.summary()

"""compile and train your new model."""

#Test Your Zaka
model2.fit(x = eng_pad, y = fr_pad, batch_size = 64, epochs=6, validation_split=0.20, callbacks=[PlotLossesKeras(), early_stop])

def download(model_type, loss, acc):
  string = f'saved_model/{model_type}.{loss}.{acc}.h5'
  # Saving the model to avoid training it again due to runtime reset:
  !mkdir -p saved_model
  # Naming convention: translation_'model architecture used'_'loss'_'acc'
  model.save(string) 
  files.download(string)

download('2BiLSTM','0.049','0.984')

"""Define a new function that relies on your new model to make predictions."""

#Test Your Zaka
# We just change the model name
def translate(model99, eng_sentence = "she is driving the truck"):
  
  # transform each text to a sequence of integers (to be used later in embeddings layer)
  # This line "sentence_token =  tokenizer_eng.texts_to_sequences(eng_sentence)" didnt work as it apply tokenization to each word or letter in the string  
  # (Based on the documentation it takes each text in texts, so it seems that it considered the input string as texts and divides it )
  # As a result, converting the string to a list [] takes the full string as 1 sentence 
  sentence_token =  tokenizer_eng.texts_to_sequences([eng_sentence])
  # print(sentence_token) # [[27, 1, 107, 5, 101]]

  sentence_pad = pad_sequences(sentence_token, maxlen=eng_max_length, padding='post')

  prediction = model99.predict(sentence_pad)
  
  prediction.shape # (1, 21, 345) = output of the Decoder
  translated_words = np.argmax(prediction, axis = 2) # argmax across axis 2 gives us (21, 345) where we have 21 max length sentence and 345 possible word
  no_zero = [i for i in translated_words[0] if i != 0] # removing zeros
  
  translated_sentence = [list(tokenizer_fr.word_index)[word - 1] for word in no_zero]
  
  translated_sentence = ' '.join(translated_sentence)
  return translated_sentence

input = "she is driving the truck"
#Test Your Zaka
translate(model2,input)

"""**Testing with only a Bidirectional layer in the encoder:**

# **Model3 = 1x Bi LSTM**
"""

#Test Your Zaka
embedding_dim = 256

model3 = Sequential()
model3.add(Embedding(eng_vocab_size, embedding_dim, input_length=eng_max_length))
model3.add(Bidirectional(LSTM(hidden_size)))
model3.add(RepeatVector(fr_max_length))
model3.add(LSTM(hidden_size, return_sequences=True))
model3.add(TimeDistributed(Dense(fr_vocab_size, activation='softmax')))

model3.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model3.summary()

#Test Your Zaka
model3.fit(x = eng_pad, y = fr_pad, batch_size = 64, epochs=6, validation_split=0.20, callbacks=[PlotLossesKeras(), early_stop])

download('1BiLSTM','0.053','0.983')

translate(model3,input)

"""# **Model4 = 1x Bi LSTM + data splitting**"""

from sklearn.model_selection import train_test_split
X = df['english']
y = df['french']
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# define your tokenizer (with num_words=10000)
tokenizer_eng = Tokenizer(num_words=1000)
tokenizer_fr = Tokenizer(num_words=1000)

# assign an index (number) to each word using fit_on_texts function  
tokenizer_eng.fit_on_texts(x_train)
tokenizer_fr.fit_on_texts(y_train)

# will be used later to pad sequences
eng_max_length = max(df.eng_length)
fr_max_length = max(df.fr_length)

# define vocabulary size
vocab_size_en = len(tokenizer_eng.word_index) + 1
vocab_size_fr = len(tokenizer_fr.word_index) + 1

# transform each text to a sequence of integers (to be used later in embeddings layer)
X_train_tokens =  tokenizer_eng.texts_to_sequences(x_train)
X_test_tokens = tokenizer_eng.texts_to_sequences(x_test)

y_train_tokens =  tokenizer_fr.texts_to_sequences(y_train)
y_test_tokens = tokenizer_fr.texts_to_sequences(y_test)

# apply post-padding to the sequences
X_train_pad = pad_sequences(X_train_tokens, maxlen=eng_max_length, padding='post')
X_test_pad = pad_sequences(X_test_tokens, maxlen=eng_max_length, padding='post')

y_train_pad = pad_sequences(y_train_tokens, maxlen=fr_max_length, padding='post')
y_test_pad = pad_sequences(y_test_tokens, maxlen=fr_max_length, padding='post')

print(X_train_pad.shape)
print(y_train_pad.shape)
print(X_test_pad.shape)
print(y_test_pad.shape)

model4 = Sequential()

model4.add(Embedding(vocab_size_en, embedding_dim, input_length=eng_max_length))

model4.add(LSTM(hidden_size))
model4.add(RepeatVector(fr_max_length))
model4.add(LSTM(hidden_size, return_sequences=True))
model4.add(Dense(32, activation='relu'))
model4.add(TimeDistributed(Dense(vocab_size_fr, activation='softmax')))
model4.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model4.summary()

model4.fit(X_train_pad, y_train_pad, batch_size=32, epochs=15, validation_data=(X_test_pad, y_test_pad), callbacks=[PlotLossesKeras()])

input = "she is driving the truck"
#Test Your Zaka
prediction = translate(model4,input)
prediction

"""# **Model5 = Stacking BiLSTMs**"""

#Test Your Zaka
embedding_dim = 128

model5 = Sequential()
model5.add(Embedding(eng_vocab_size, embedding_dim, input_length=eng_max_length))
model5.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))
model5.add(Bidirectional(LSTM(hidden_size)))
model5.add(RepeatVector(fr_max_length))
model5.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))
model5.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))
model5.add(TimeDistributed(Dense(fr_vocab_size, activation='softmax')))

model5.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model5.summary()

#Test Your Zaka
model5.fit(x = eng_pad, y = fr_pad, batch_size = 64, epochs=6, validation_split=0.20, callbacks=[PlotLossesKeras(), early_stop])

download('1BiLSTM','0.053','0.983')

translate(model5)

"""# Models Comparison:
- Model1 = normal LSTM                     |  **elle conduit le le lac**
- Model2 = 2x Bi LSTM               | **elle conduit le camion camion**
- Model3 = 1x Bi LSTM                  |  **elle conduit le nouveau camion**
- Model4 = 1x Bi LSTM + data splitting                          | **elle conduit le camion camion blanc**
- Model5 = Stacking BiLSTMs | **elle conduit le petit camion noir**

Note: It seems like the models predict the main words but it prints with it an adjective such as nouveau ot blanc.

**What is another adjustment in terms of architecture that you might be able to do to improve your model?**

- Attention (or using Transformers)
- Using GRU may have a better performance than LSTM

**What are some additional ways that we can do to improve the performance of our model?**

- Splitting data for training and testing 
- More data cleaning. Maybe removing stopwatch, keep the ' in the french language and other things that may help improving the input data
- Playing with the Deep Layers number
- More epoch as we can see that there is a rising trend for the model

# **Testing Attention**
"""

from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Concatenate
from tensorflow.keras import Input, Model

# Encoder input
encoder_inputs = Input(shape=(eng_max_length,)) 

# Embedding layer- i am using 1024 output-dim for embedding you can try diff values 100,256,512,1000
enc_emb = Embedding(eng_vocab_size, 1024)(encoder_inputs)

# Bidirectional lstm layer
enc_lstm1 = Bidirectional(LSTM(256,return_sequences=True,return_state=True))
encoder_outputs1, forw_state_h, forw_state_c, back_state_h, back_state_c = enc_lstm1(enc_emb)

# Concatenate both h and c 
final_enc_h = Concatenate()([forw_state_h,back_state_h])
final_enc_c = Concatenate()([forw_state_c,back_state_c])

# get Context vector
encoder_states =[final_enc_h, final_enc_c]

from tensorflow.keras.layers import Attention
#  decoder input
decoder_inputs = Input(shape=(None,)) 

# decoder embedding with same number as encoder embedding
dec_emb_layer = Embedding(fr_vocab_size, 1024) 
dec_emb = dec_emb_layer(decoder_inputs)   # apply this way because we need embedding layer for prediction 

# In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm
# LSTM using encoder's final states as initial state
decoder_lstm = LSTM(512, return_sequences=True, return_state=True) 
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)

# Using Attention Layer
attention_layer = AttentionLayer()
attention_result = attention_layer([encoder_outputs1, decoder_outputs])

# Concat attention output and decoder LSTM output 
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])

# Dense layer with softmax
decoder_dense = Dense(fr_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_concat_input)
dropout = Dropout(rate=0.20)
decoder_outputs = dropout(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# ***IMP note :- if loss categorical crossentropy used then shapes incompatible error will occcur beause 
# we have to use sparse_categorical_crossentropy when we have all different labels categorical is for mutliclass labels***

# compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define callbacks
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
checkpoint = ModelCheckpoint("give Your path to save check points", monitor='val_accuracy')
early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)
callbacks_list = [checkpoint, early_stopping]

# Training set
encoder_input_data = X_train_pad
# To make same as target data skip last number which is just padding
decoder_input_data = y_train_pad[:,:-1]
# Decoder target data has to be one step ahead so we are taking from 1 as told in keras docs
decoder_target_data =  y_train_pad[:,1:]

# devlopment set
encoder_input_test = X_test_pad
decoder_input_test = y_test_pad[:,:-1]
decoder_target_test=  y_test_pad[:,1:]

EPOCHS= 4 #@param {type:'slider',min:0,max:10, step:1}
history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data, 
                    epochs=EPOCHS, 
                    batch_size=128,
                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),
                    callbacks= callbacks_list)

# INFERENCE MODEL
# encoder Inference model
encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])

# Decoder Inference
decoder_state_h = Input(shape=(512,)) # This numbers has to be same as units of lstm's on which model is trained
decoder_state_c = Input(shape=(512,))

# we need hidden state for attention layer
# 36 is maximum length if english sentence It has to same as input taken by attention layer can see in model plot
decoder_hidden_state_input = Input(shape=(15,512)) 
# get decoder states
dec_states = [decoder_state_h, decoder_state_c]

# embedding layer 
dec_emb2 = dec_emb_layer(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)

# Attention inference
attention_result_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])
decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])

dec_states2= [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_concat_input_inf)

# get decoder model
decoder_model= Model(
                    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],
                     [decoder_outputs2]+ dec_states2)

def get_predicted_sentence(input_seq):
    # Encode the input as state vectors.
    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)
  
    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1,1))
    
    # Populate the first character of target sequence with the start character.
    # target_seq[0, 0] = mar_word_index['sos']
    
    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])
        
        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        # print(f'sampled_token_index {sampled_token_index}')
        if sampled_token_index == 0:
          break
        # convert max index number to marathi word
        sampled_char = fr_index_word[sampled_token_index]
        # print(f'sampled_char: {sampled_char}')
        # aapend it to decoded sent
        decoded_sentence += ' '+sampled_char
        # print(f'decoded_sentence: {decoded_sentence}')

        # Exit condition: either hit max length or find stop token.
        if (sampled_char == 'eos' or len(decoded_sentence.split()) >= vocab_size_fr):
            stop_condition = True
        
        # Update the target sequence (of length 1).
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index
        
        # Update states
        enc_h, enc_c = h, c
    
    return decoded_sentence

# English Word --> index dictionary
eng_index_word = tokenizer_eng.index_word
eng_word_index = tokenizer_eng.word_index
# French Index --> word dict
fr_index_word = tokenizer_fr.index_word
fr_word_index = tokenizer_fr.word_index
# we need actual French sentence so crate fuction to convert back tokens into words
def get_French_sentence(input_sequence):
    sentence =''
    for i in input_sequence:
      if i!=0 :
        sentence =sentence +fr_index_word[i]+' '
    return sentence 

# same as above we want input english sentence so create function
def get_english_sentence(input_sequence):
    sentence =''
    for i in input_sequence:
      if i!=0:
        sentence =sentence +eng_index_word[i]+' '
    return sentence     

# using simple loop we will take 15 random numbers from x_test and get results
for i in np.random.randint(10, 1000, size=15):
  print("English Sentence:",get_english_sentence(X_test_pad[i]))
  print("Actual French Sentence:",get_French_sentence(y_test_pad[i])[4:-4])
  # Before passing input it has to be reshape as following
  print("Predicted French Translation:",get_predicted_sentence(X_test_pad[i].reshape(1,eng_max_length))[:-4])
  print("----------------------------------------------------------------------------------------")

get_predicted_sentence(X_test_pad[0].reshape(1,eng_max_length))

pred = pd.DataFrame(["she is driving the truck"], columns=['sen_to_pred'])
p = pred['sen_to_pred']
p_tk = tokenizer_eng.texts_to_sequences(p)
p_pad = pad_sequences(p_tk, maxlen=eng_max_length, padding='post')
get_predicted_sentence(p_pad[0].reshape(1,eng_max_length))

"""It is not translating well words that are new. We dont know yet what is the main problem. It may be overfit as it reached more than 90% acc in the second epoch only as acc is not a reliable metric for NLP applications. To solve it a bit we introduced a Dropout Layer. (When we added the droupout, we used a rate of 0.3, the learning became so slow so we reduced it)

Maybe using Teacher Forcing would sldo help the training.

We noticed that the validation accuracy is much higher than the training accuracy.

At least we tried. 😀 🚀
"""